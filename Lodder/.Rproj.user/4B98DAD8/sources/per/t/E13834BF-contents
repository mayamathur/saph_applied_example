
## Overview
 
This repository contains all data, code, and materials required to reproduce the code example, applied examples, and simulation study reported in:

*Mathur MB (under review). Sensitivity analysis for p-hacking in meta-analyses.*


## How to reproduce the simulation analyses from our saved results

The key variable names for the reported performance metrics are:

- `Phat`: the estimated percentage of effects stronger than q for moderator level z
- `PhatLo` and `PhatHi`: confidence interval limits for the above
- `TheoryP`: estimand of the above (i.e., the true percentage)
- `PhatBias`: bias of the above
- `PhatAbsErr`: absolute error of the above
- `PhatWidth`: width of confidence intervals for the above
- `CoverPhat`: coverage of the above

- `Diff`: the estimated difference in percentages of effects stronger than q for moderator level z vs. z0
- `DiffLo` and `DiffHi`: confidence interval limits for the above
- `CoverDiff`: coverage for the above
- `TheoryDiff`: estimand of the above (i.e., the true difference)
- `DiffBias`: bias of the above
- `DiffAbsErr`: absolute error of the above
- `DiffWidth`: width of confidence intervals for the above
- `DiffPhat`: coverage of the above

Other results files:

- `results_sanchecks.xlsx`: For each scen, shows just the sanity check variables in agg dataset 


## How to reproduce the applied example (Lodder)

All in `Applied examples/Lodder`:
- `1_prep_lodder.R`
- `2_analyze_lodder_sherlock.R` makes `results_lodder.csv`
- `3_anaylze_lodder_local.R` makes Z-score density plot, forest plot, looks at SE distribution

# FROM MRM: 
## General notes on the code

- In some files, there is a global variable such as `overwrite.existing.prepped`, `overwrite.results`, and `bootstrap.plots.from.scratch` that controls whether auto-written results files should be overwritten or not. By default, such variables are set to `FALSE` to avoid accidental overwriting, especially because some results take a long time to generate and accidentally wiping them would wipe the numbers being piped into the reproducibly written manuscript. To reproduce the results and also overwrite any existing results files, set these variables to `TRUE`.

## How to run the reproducible example with fake data

From the directory __Code (git)/Reproducible example__, download the fake dataset `reprex_fake_data.csv` and the analysis script `simple_reprex.R`, which is thoroughly commented. Set your working directory to the folder that contains `reprex_fake_data.csv`, then simply run `simple_reprex.R`. 


## How to reproduce the applied examples on memory consolidation and meat consumption

The data from [Hu et al. (2010)](https://osf.io/n3x6t/) and from [Mathur et al. (in press)](https://osf.io/5zyvt/) are publicly available. We used the scripts [prep_applied_MRM.R](https://osf.io/9nct8/) to prepare the datasets for analysis and then used the script [analyze_applied_MRM.R](https://osf.io/8sw9j/) to conduct analyses. That script calls helper functions in [helper_applied_MRM.R](https://osf.io/awytm/) as well as [helper_MRM.R](https://osf.io/7skuv/) and [bootfuns.R](https://osf.io/v6nxe/) from the simulation study.



## How to reproduce the simulation analyses from our saved results

All results for the simulation study are available (directory __Simulation study results__). The directories whose names start with "Supplement" contain secondary results that were actually reported in the main text, not the supplement. The directory __Supplement - boot-correct meta-regression__ contains code and results from the secondary analysis of the 32 worst scenarios. The directories are as follows: 

- __**Main text sims__: code and results from the main simulations of all 4,800 scenarios, including bootstrap bias corrections applied directly to the proposed estimators

- __**Supplement - boot-correct meta-regression__: code and results from the secondary simulations of the 32 worst scenarios, including bootstrap bias corrections applied to the meta-regression estimates 

- __**Supplement - 10K iterates__: code and results from running a handful of scenarios with 10K bootstrap iterates instead of 1K; aborted because the results didn't change

We used the script [data_prep_sims_MRM.R](https://osf.io/zw24m/) to clean the raw results files and aggregate by scenario This handles the main simulations as well as the secondary ones, and it calls [helper_MRM.R](https://osf.io/7skuv/). For the main simulations, the script creates the analysis datasets:

- [s3_dataset.csv](https://osf.io/n2q3f/): results at the level of the individual simulation iterate (has approximately 4800 scenarios * 500 iterates rows), not yet aggregated by scenario but with performance metrics added to the raw data

- [*agg_dataset.csv](https://osf.io/eg9sq/): results aggregated by scenario (has approximately 4800 rows). __This is the most important dataset for the analyses and is also a convenient way to view comprehensive results for each scenario individually. This dataset includes all scenarios, even those that do not fulfill the guidelines given in the paper.__

- [*agg_dataset_recommended_scens_phat.csv](https://osf.io/arwyz/) and [*agg_dataset_recommended_scens_diff.csv](https://osf.io/6fjxu/): same as `*agg_dataset` above, but __restricted to scenarios following the reporting guidelines we give in the paper for each metric.__


Then we used [analysis_sims_MRM.R](https://osf.io/yqzcj/) to conduct all analyses and to produce the tables. This script also handles the main simulations and the secondary ones.

The results, including autogenerated versions of the tables in the paper, are in __**Main text sims__. __For extended versions of the papers in the table with additional performance metrics, see [extended_performance_table.csv](https://osf.io/45mgv/)__.

The key variable names for the reported performance metrics are:

- `Phat`: the estimated percentage of effects stronger than q for moderator level z
- `PhatLo` and `PhatHi`: confidence interval limits for the above
- `TheoryP`: estimand of the above (i.e., the true percentage)
- `PhatBias`: bias of the above
- `PhatAbsErr`: absolute error of the above
- `PhatWidth`: width of confidence intervals for the above
- `CoverPhat`: coverage of the above

- `Diff`: the estimated difference in percentages of effects stronger than q for moderator level z vs. z0
- `DiffLo` and `DiffHi`: confidence interval limits for the above
- `CoverDiff`: coverage for the above
- `TheoryDiff`: estimand of the above (i.e., the true difference)
- `DiffBias`: bias of the above
- `DiffAbsErr`: absolute error of the above
- `DiffWidth`: width of confidence intervals for the above
- `DiffPhat`: coverage of the above




## How to re-run the simulation study from scratch

Simulation scripts are parallelized and were run on a SLURM cluster. The most updated versions of the simulation scripts are in __Code (git)/Simulation study__. These scripts arose from updating scripts from the main simulations to accommodate the various secondary simulations. The scripts should be backward-compatible such that the main simulations can still be reproduced from them, but just in case, the various simulation results folders also contain archives of the code as it was when we ran that particular batch of simulations (equivalently, you can go through the commit history on git). The most important scripts are:

- `helper_MRM.R` and `bootfuns_MRM.R` contains documented helper functions that can be run locally. This is the file to consult if you have questions about the various custom functions used in `doParallel_MRM.R` below. 

- `doParallel_MRM.R` runs a parallelized simulation study. Specifically, it runs `sim.reps=500` simulation reps on each computing node. Each scenario's total simulation reps that were spread across multiple sbatch files. For each simulation rep, this script generates an original dataset and conducts the analyses by calling `helper_MRM.R` and `bootfuns_MRM.R`. This script can either be run locally or on a SLURM cluster. 

- `genSbatch_SAPB.R` automatically generates the sbatch files based on user-specified simulation parameters (from which `genSbatch_SAPB.R` writes a spreadsheet of scenario parameters, `scen_params.csv`, to the cluster environment; this file is called later by `doParallel.R`). This script is specialized for our cluster and would likely need to be rewritten for other computing systems. 

- `stitch_on_sherlock.R` takes output files written by `doParallel_SAPB.R` and stitches them into a single file, [stitched.csv](https://osf.io/tvnsb/), that is used for analysis. 


We ran the simulations as follows:

1. We ran the main simulation study with all 4,800 scenarios. Some simulations reps failed to produce results on the first run because of errors like `Lapack routine dgesv` in `robumeta` (i.e., the generated dataset was too extreme in some way to fit the meta-regression), so we reran missed job numbers to fill in these initial failures. We did this about 5 times until almost all simulation reps had been completed.  

2. As described above, we used `data_prep_sims_MRM.R` to aggregate the above data into `*agg_dataset.csv` and then used [choose_worst_scens.R](https://osf.io/jgvwp/) to choose the worst-performing scenarios for the bias correction follow-up simulation study including methods `calib.method = 'MR'` (again), `calib.method = 'params'`, and `calib.method = 'MR bt both correct'`.  



## Miscellaneous

For a simulated example demonstrating that the one-stage and two-stage methods may not be exactly numerically equivalent, see [are_one_stage_two_stage_equivalent.R](https://osf.io/cnhf3/).


